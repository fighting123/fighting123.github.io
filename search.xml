<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA['node-crwl-lagou']]></title>
    <url>%2F2018%2F07%2F05%2Fnode-crwl-lagou%2F</url>
    <content type="text"><![CDATA[前言 之前断断续续学习了node.js，今天就拿拉勾网练练手，顺便通过数据了解了解最近的招聘行情哈！node方面算是萌新一个吧，希望可以和大家共同学习和进步。 一、概要我们首先需要明确具体的需求： 可以通过node index 城市 职位来爬取相关信息 也可以输入node index start直接爬取我们预定义好的城市和职位数组，循环爬取不同城市的不同职位信息 将最终爬取的结果存储在本地的./data目录下 生成对应的excel文件，并存储到本地 二、爬虫用到的相关模块 fs: 用于对系统文件及目录进行读写操作 async：流程控制 superagent：客户端请求代理模块 node-xlsx：将一定格式的文件导出为excel 三、爬虫主要步骤：初始化项目新建项目目录 在合适的磁盘目录下创建项目目录 node-crwl-lagou 初始化项目 进入node-crwl-lagou文件夹下 执行npm init，初始化package.json文件 安装依赖包 npm install async npm install superagent npm install node-xlsx 命令行输入的处理对于在命令行输入的内容，可以用process.argv来获取，他会返回个数组，数组的每一项就是用户输入的内容。区分node index 地域 职位和node index start两种输入，最简单的就是判断process.argv的长度，长度为四的话，就直接调用爬虫主程序爬取数据，长度为三的话，我们就需要通过预定义的城市和职位数组来拼凑url了，然后利用async.mapSeries循环调用主程序。关于命令分析的主页代码如下： 1234567891011121314151617181920212223if (process.argv.length === 4) &#123; let args = process.argv console.log(&apos;准备开始请求&apos; + args[2] + &apos;的&apos; + args[3] + &apos;职位数据&apos;); requsetCrwl.controlRequest(args[2], args[3])&#125; else if (process.argv.length === 3 &amp;&amp; process.argv[2] === &apos;start&apos;) &#123; let arr = [] for (let i = 0; i &lt; defaultArgv.city.length; i++) &#123; for (let j = 0; j &lt; defaultArgv.position.length; j++) &#123; let obj = &#123;&#125; obj.city = defaultArgv.city[i] obj.position = defaultArgv.position[j] arr.push(obj) &#125; &#125; async.mapSeries(arr, function (item, callback) &#123; console.log(&apos;准备开始请求&apos; + item.city + &apos;的&apos; + item.position + &apos;职位数据&apos;); requsetCrwl.controlRequest(item.city, item.position, callback) &#125;, function (err) &#123; if (err) throw err &#125;)&#125; else &#123; console.log(&apos;请正确输入要爬取的城市和职位，正确格式为：&quot;node index 城市 关键词&quot; 或 &quot;node index start&quot; 例如：&quot;node index 北京 php&quot; 或&quot;node index start&quot;&apos;)&#125; 预定义好的城市和职位数组如下： 1234&#123; &quot;city&quot;: [&quot;北京&quot;,&quot;上海&quot;,&quot;广州&quot;,&quot;深圳&quot;,&quot;杭州&quot;,&quot;南京&quot;,&quot;成都&quot;,&quot;西安&quot;,&quot;武汉&quot;,&quot;重庆&quot;], &quot;position&quot;: [&quot;前端&quot;,&quot;java&quot;,&quot;php&quot;,&quot;ios&quot;,&quot;android&quot;,&quot;c++&quot;,&quot;python&quot;,&quot;.NET&quot;]&#125; 接下来就是爬虫主程序部分的分析了。 分析页面，找到请求地址首先我们打开拉勾网首页，输入查询信息（比如node），然后查看控制台，找到相关的请求，如图： 这个post请求https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false就是我们所需要的，通过三个请求参数来获取不同的数据，简单的分析就可得知：参数first是标注当前是否是第一页，true为是，false为否；参数pn是当前的页码；参数kd是查询输入的内容。 通过superagent请求数据首先需要明确得是，整个程序是异步的，我们需要用async.series来依次调用。查看分析返回的response： 可以看到content.positionResult.totalCount就是我们所需要的总页数我们用superagent直接调用post请求，控制台会提示如下信息： {&apos;success&apos;: False, &apos;msg&apos;: &apos;您操作太频繁,请稍后再访问&apos;, &apos;clientIp&apos;: &apos;122.xxx.xxx.xxx&apos;} 这其实是反爬虫策略之一，我们只需要给其添加一个请求头即可，请求头的获取方式很简单，如下： 然后在用superagent调用post请求，主要代码如下： 12345678910111213141516171819202122// 先获取总页数 (cb) =&gt; &#123; superagent .post(`https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&amp;city=$&#123;city&#125;&amp;kd=$&#123;position&#125;&amp;pn=1`) .send(&#123; &apos;pn&apos;: 1, &apos;kd&apos;: position, &apos;first&apos;: true &#125;) .set(options.options) .end((err, res) =&gt; &#123; if (err) throw err // console.log(res.text) let resObj = JSON.parse(res.text) if (resObj.success === true) &#123; totalPage = resObj.content.positionResult.totalCount; cb(null, totalPage); &#125; else &#123; console.log(`获取数据失败:$&#123;res.text&#125;&#125;`) &#125; &#125;) &#125;, 拿到总页数后，我们就可以通过总页数/15获取到pn参数，循环生成所有url并存入urls中： 1234567(cb) =&gt; &#123; for (let i=0;Math.ceil(i&lt;totalPage/15);i++) &#123; urls.push(`https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&amp;city=$&#123;city&#125;&amp;kd=$&#123;position&#125;&amp;pn=$&#123;i&#125;`) &#125; console.log(`$&#123;city&#125;的$&#123;position&#125;职位共$&#123;totalPage&#125;条数据，$&#123;urls.length&#125;页`); cb(null, urls); &#125;, 有了所有的url，在想爬到所有的数据就不是难事了，继续用superagent的post方法循环请求所有的url，每一次获取到数据后，在data目录下创建json文件，将返回的数据写入。这里看似简单，但是有两点需要注意： 为了防止并发请求太多而导致被封IP：循环url时候需要使用async.mapLimit方法控制并发为3， 每次请求完都要过两秒在发送下一次的请求 在async.mapLimit的第四个参数中，需要通过判断调用主函数的第三个参数是否存在来区分一下是那种命令输入，因为对于node index start这个命令，我们使用得是async.mapSeries，每次调用主函数都传递了(city, position, callback)，所以如果是node index start的话，需要在每次获取数据完后将null传递回去，否则无法进行下一次循环 主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 控制并发为3 (cb) =&gt; &#123; async.mapLimit(urls, 3, (url, callback) =&gt; &#123; num++; let page = url.split(&apos;&amp;&apos;)[3].split(&apos;=&apos;)[1]; superagent .post(url) .send(&#123; &apos;pn&apos;: totalPage, &apos;kd&apos;: position, &apos;first&apos;: false &#125;) .set(options.options) .end((err, res) =&gt; &#123; if (err) throw err let resObj = JSON.parse(res.text) if (resObj.success === true) &#123; console.log(`正在抓取第$&#123;page&#125;页，当前并发数量：$&#123;num&#125;`); if (!fs.existsSync(&apos;./data&apos;)) &#123; fs.mkdirSync(&apos;./data&apos;); &#125; // 将数据以.json格式储存在data文件夹下 fs.writeFile(`./data/$&#123;city&#125;_$&#123;position&#125;_$&#123;page&#125;.json`, res.text, (err) =&gt; &#123; if (err) throw err; // 写入数据完成后，两秒后再发送下一次请求 setTimeout(() =&gt; &#123; num--; console.log(`第$&#123;page&#125;页写入成功`); callback(null, &apos;success&apos;); &#125;, 2000); &#125;); &#125; &#125;) &#125;, (err, result) =&gt; &#123; if (err) throw err; // 这个arguments是调用controlRequest函数的参数，可以区分是那种爬取（循环还是单个） if (arguments[2]) &#123; ok = 1; &#125; cb(null, ok) &#125;) &#125;, () =&gt; &#123; if (ok) &#123; setTimeout(function () &#123; console.log(`$&#123;city&#125;的$&#123;position&#125;数据请求完成`); indexCallback(null); &#125;, 5000); &#125; else &#123; console.log(`$&#123;city&#125;的$&#123;position&#125;数据请求完成`); &#125; // exportExcel.exportExcel() // 导出为excel &#125; 导出的json文件如下： json文件导出为excel将json文件导出为excel有多种方式，我使用的是node-xlsx这个node包，这个包需要将数据按照固定的格式传入，然后导出即可，所以我们首先做的就是先拼出其所需的数据格式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869function exportExcel() &#123; let list = fs.readdirSync(&apos;./data&apos;) let dataArr = [] list.forEach((item, index) =&gt; &#123; let path = `./data/$&#123;item&#125;` let obj = fs.readFileSync(path, &apos;utf-8&apos;) let content = JSON.parse(obj).content.positionResult.result let arr = [[&apos;companyFullName&apos;, &apos;createTime&apos;, &apos;workYear&apos;, &apos;education&apos;, &apos;city&apos;, &apos;positionName&apos;, &apos;positionAdvantage&apos;, &apos;companyLabelList&apos;, &apos;salary&apos;]] content.forEach((contentItem) =&gt; &#123; arr.push([contentItem.companyFullName, contentItem.phone, contentItem.workYear, contentItem.education, contentItem.city, contentItem.positionName, contentItem.positionAdvantage, contentItem.companyLabelList.join(&apos;,&apos;), contentItem.salary]) &#125;) dataArr[index] = &#123; data: arr, name: path.split(&apos;./data/&apos;)[1] // 名字不能包含 \ / ? * [ ] &#125; &#125;)// 数据格式// var data = [// &#123;// name : &apos;sheet1&apos;,// data : [// [// &apos;ID&apos;,// &apos;Name&apos;,// &apos;Score&apos;// ],// [// &apos;1&apos;,// &apos;Michael&apos;,// &apos;99&apos;//// ],// [// &apos;2&apos;,// &apos;Jordan&apos;,// &apos;98&apos;// ]// ]// &#125;,// &#123;// name : &apos;sheet2&apos;,// data : [// [// &apos;AA&apos;,// &apos;BB&apos;// ],// [// &apos;23&apos;,// &apos;24&apos;// ]// ]// &#125;// ]// 写xlsx var buffer = xlsx.build(dataArr) fs.writeFile(&apos;./result.xlsx&apos;, buffer, function (err) &#123; if (err) throw err; console.log(&apos;Write to xls has finished&apos;);// 读xlsx// var obj = xlsx.parse(&quot;./&quot; + &quot;resut.xls&quot;);// console.log(JSON.stringify(obj)); &#125; );&#125; 导出的excel文件如下，每一页的数据都是一个sheet，比较清晰明了： 我们可以很清楚的从中看出目前西安.net的招聘情况，之后也可以考虑用更形象的图表方式展示爬到的数据，应该会更加直观！ 总结其实整个爬虫过程并不复杂，注意就是注意的小点很多，比如async的各个方法的使用以及导出设置header等，总之，也是收获满满哒！ 源码gitbug地址： https://github.com/fighting123/node_crwl_lagou]]></content>
      <categories>
        <category>打包</category>
      </categories>
      <tags>
        <tag>webpack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webpack初识]]></title>
    <url>%2F2018%2F03%2F19%2Fwebpack%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[安装webpack123456789//全局安装npm install -g webpack//安装到项目目录npm install --save-dev webpack//全局安装（同时需要安装webpack-cli，否则提示The CLI moved into a separate package: webpack-cli.）npm install webpack-cli -g//安装到项目目录npm install --save-dev webpack-cli 区别：安装到项目目录运行时需要加上在node_modules中的地址，如：node_modules/.bin/webpack 使用webpack前的准备工作创建如图所示的文件： 首先用npm init生成package.json文件，然后创建文件，public是存放浏览器读取的文件，app是存放未打包前的模块文件 Greeter.js模块: 1234567const greeter = () =&gt; &#123; let greet = document.createElement(&apos;div&apos;) greet.textContent = &apos;hello&apos; return greet&#125;module.exports = greeter() main.js引入模块的文件： 12let greeter = require(&apos;./Greeter&apos;)document.querySelector(&quot;#body&quot;).appendChild(greeter) index.html主页面: 12345678910111213&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;webpack_sample_practice&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;body&quot;&gt;&lt;/body&gt;&lt;script src=&quot;bundle.js&quot;&gt;&lt;/script&gt;&lt;/html&gt; bundle.js打包后存储的文件（index.html引入的文件） （一）使用命启动令webpack1webpack app/main.js public/bundle.js // webpack 入口文件 打包后文件 这时会提示warning，这是因为没有设置mode，每次启动的时候：123webpack app/main.js public/bundle.js --mode development（未压缩）// 或者webpack app/main.js public/bundle.js --mode production（压缩过） 为方便起见，我们将这两句命令放入package.json文件中：1234&quot;scripts&quot;: &#123; &quot;dev&quot;: &quot;webpack app/main.js public/bundle.js --mode development&quot;,//（未压缩） &quot;build&quot;: &quot;webpack app/main.js public/bundle.js --mode production&quot; //（压缩过） &#125;, 这样，在每次启动是只需要输入npm run dev 或者npm run build 最后，打开浏览器页面就可以访问啦！ （二）通过配置webpack.config.js来使用webpack我们也可以通过更加方便简洁的方式使用webpack，这样也比较不容易出错。 在主目录新建webpack.config.js配置文,先简单的只配置下入口和出口文件目录: 1234567module.exports = &#123; entry: __dirname + &quot;/app/main.js&quot;,//已多次提及的唯一入口文件 output: &#123; path: __dirname + &quot;/public&quot;,//打包后的文件存放的地方 filename: &quot;bundle.js&quot;//打包后输出文件的文件名 &#125;&#125; 同意为方便我们配置package.json文件： 1234&quot;scripts&quot;: &#123; &quot;dev&quot;: &quot;webpack --mode development&quot;, &quot;build&quot;: &quot;webpack --mode production&quot; &#125;, 输入npm run dev 或者 npm run build同样可以正常启动啦！ 参考：https://www.jianshu.com/p/42e11515c10f]]></content>
      <categories>
        <category>打包</category>
      </categories>
      <tags>
        <tag>webpack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F03%2F15%2Fhello-world-2%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 这一路上走走停停 顺着少年漂流的痕迹 迈出车站的前一刻 竟有些犹豫 不禁笑这近乡情怯 仍无可避免而长野的天 依旧那么暖 风吹起了从前 从前初识这世间 万般流连 看着天边似在眼前 也甘愿赴汤蹈火去走它一遍如今走过这世间 万般流连 翻过岁月不同侧脸 措不及防闯入你的笑颜 我曾难自拔于世界之大也沉溺于其中梦话 不得真假 不做挣扎 不惧笑话 我曾将青春翻涌成她 也曾指尖弹出盛夏心之所动 且就随缘去吧 逆着光行走 任风吹雨打 短短的路走走停停 也有了几分的距离 不知抚摸的是故事 还是段心情 也许期待的不过是 与时间为敌再次看到你 微凉晨光里 笑的很甜蜜 从前初识这世间万般流连 看着天边似在眼前 也甘愿赴汤蹈火去走它一遍 如今走过这世间 万般流连 翻过岁月不同侧脸措不及防闯入你的笑颜 我曾难自拔于世界之大 也沉溺于其中梦话 不得真假 不做挣扎 不惧笑话 我曾将青春翻涌成她也曾指尖弹出盛夏 心之所动 且就随缘去吧 晚风吹起你鬓间的白发 抚平回忆留下的疤 你的眼中 明暗交杂 一笑生花暮色遮住你蹒跚的步伐 走进床头藏起的画 画中的你 低着头说话 我仍感叹于世界之大 也沉醉于儿时情话不剩真假 不做挣扎 无谓笑话 我终将青春还给了她 连同指尖弹出的盛夏 心之所动 就随风去了 以爱之名 你还愿意吗 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>initTest2</category>
      </categories>
      <tags>
        <tag>test2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F03%2F14%2Fhello-world%2F</url>
    <content type="text"><![CDATA[博客搭建步骤简记 按照教程配好基础的博客并修改个人信息 更换主题为next 添加分类。标签功能 添加搜索功能，next官网上有很多第三方搜索服务，我用的是Local Search，具体可以按照教程来 添加统计，同上，我用的不蒜子 添加评论功能，同上，我用的是DISQUS(参考：https://www.jianshu.com/p/d68de067ea74?open_source=weibo_search) 首页文章以摘要形式显示：将主题设置的auto_excerpt的enable为true 设置首页文章显示篇数(参考：http://www.jeyzhang.com/next-theme-personal-settings.html) 设置404页面,用的是腾讯公益404页面(直接在站点的source目录下新建404.html) 设置头像(两张存储位置对应不用的文件夹名称，需要注意，然后将主题的avatar的注释放开并且填写对应位置) 解决github+Hexo的博客多终端同步问题(https://www.jianshu.com/p/6fb0b287f950)]]></content>
      <categories>
        <category>initTest1</category>
      </categories>
      <tags>
        <tag>test1</tag>
      </tags>
  </entry>
</search>
