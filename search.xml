<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[vue单页面：当前页面刷新或跳转时提示保存]]></title>
    <url>%2F2018%2F11%2F01%2Fvue%E5%8D%95%E9%A1%B5%E9%9D%A2%EF%BC%9A%E5%BD%93%E5%89%8D%E9%A1%B5%E9%9D%A2%E5%88%B7%E6%96%B0%E6%88%96%E8%B7%B3%E8%BD%AC%E6%97%B6%E6%8F%90%E7%A4%BA%E4%BF%9D%E5%AD%98%2F</url>
    <content type="text"><![CDATA[前言 最近公司vue项目中有一个需求，需要在当前页面刷新或跳转时提示保存并可取消刷新，以防止填写的表单内容丢失。刚开始思考觉得很简单，直接在Router的钩子中判断就好了，但是会发现还有新的问题存在，浏览器刷新和当前页面关闭的时候无法监听，最终用window.onbeforeunload成功解决，所以用这篇文章简单记录下整个解决过程。 vue-Router的钩子：路由钩子可以分为全局的，单个路由独享的以及组件级别的，解决上述需求只用到了组件级别的路由钩子，所以本文只介绍组件级别的路由钩子，全局的和单个路由独享的路由钩子有需要的同学可以去vue-router官网查看介绍： 组件级别路由钩子分为三种： beforeRouteEnter：当成功获取并能进入路由(在渲染该组件的对应路由被 confirm 前) beforeRouteUpdate：在当前路由改变，但是该组件被复用时调用 beforeRouteLeave：导航离开该组件的对应路由时调用 具体的介绍和写法如下：1234567891011121314151617181920212223242526272829303132const Foo = &#123; template: `...`, beforeRouteEnter (to, from, next) &#123; // 在渲染该组件的对应路由被 confirm 前调用 // 不！能！获取组件实例 `this` // 因为当守卫执行前，组件实例还没被创建 // 可以通过传一个回调给 next来访问组件实例 next(vm =&gt; &#123; // 通过 `vm` 访问组件实例 &#125;) &#125;, beforeRouteUpdate (to, from, next) &#123; // 在当前路由改变，但是该组件被复用时调用 // 举例来说，对于一个带有动态参数的路径 /foo/:id，在 /foo/1 和 /foo/2 之间跳转的时候， // 由于会渲染同样的 Foo 组件，因此组件实例会被复用。而这个钩子就会在这个情况下被调用。 // 可以访问组件实例 `this` // 不支持传递回调(因为this实例已经创建可以获取到，所以没必要) next() &#125;, beforeRouteLeave (to, from, next) &#123; // 导航离开该组件的对应路由时调用 // 可以访问组件实例 `this` // 该导航可以通过 next(false) 来取消。 const answer = window.confirm(&apos;Do you really want to leave? you have unsaved changes!&apos;) if (answer) &#123; next() &#125; else &#123; // 不支持传递回调(因为this实例已经创建可以获取到，所以没必要) next(false) &#125; &#125;&#125; 注意：在刷新当前页面时候，beforeRouteLeave不会触发，它只在进入到其他页面时候才会触发，但是beforeRouteEnter会在刷新的时候触发。 通过beforeRouteLeave这个路由钩子，我们就可以在用户要离开此页面时候进行提示了！ 12345678beforeRouteLeave (to, from, next) &#123; const answer = window.confirm(&apos;当前页面数据未保存，确定要离开![image](http://note.youdao.com/favicon.ico)？&apos;) if (answer) &#123; next() &#125; else &#123; next(false) &#125; &#125; 显示的提示框如下： 监听浏览器的刷新、页面关闭事件但是，这个时候就实现了我们最终的需求了么？并没有，还有一步：用window.onbeforeunload监听浏览器的刷新事件，当然为了防止从当前单页面跳到其他页面之后，在刷新所在新的页面还会触发window上的onbeforeunload的问题，我们不仅要及时的添加onbeforeunload事件，更要及时删除此事件，下面有两种解决方法可供选择： 通过判断它的路由是否是当前需要添加禁止刷新的页面 123456789101112131415mounted() &#123; window.onbeforeunload = function (e) &#123; if(_this.$route.fullPath ==&quot;/layout/add&quot;)&#123; e = e || window.event; // 兼容IE8和Firefox 4之前的版本 if (e) &#123; e.returnValue = &apos;关闭提示&apos;; &#125; // Chrome, Safari, Firefox 4+, Opera 12+ , IE 9+ return &apos;关闭提示&apos;; &#125;else&#123; window.onbeforeunload =null &#125;&#125;&#125;; 在destory或者beforeDestory的生命周期中直接将onbeforeunload事件置空1234567891011121314mounted() &#123; window.onbeforeunload = function (e) &#123; e = e || window.event; // 兼容IE8和Firefox 4之前的版本 if (e) &#123; e.returnValue = &apos;关闭提示&apos;; &#125; // Chrome, Safari, Firefox 4+, Opera 12+ , IE 9+ return &apos;关闭提示&apos;; &#125;&#125;;destroyed() &#123; window.onbeforeunload = null &#125; 显示的提示框如下： 总结最终，在beforeRouteLeave和onbeforeunload的共同作用下，这个刷新、跳转或者关闭等情况下需要提示保存的需求完美解决！但是，还有一点点小遗憾，就是onbeforeunload中弹框的自定义提示语设置始终无法生效，大家要是有更加合适的处理办法，欢迎多多交流指正！]]></content>
      <categories>
        <category>vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
        <tag>vue-router刷新</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[node-login-segmentfault]]></title>
    <url>%2F2018%2F07%2F10%2Fnode-login-segmentfault%2F</url>
    <content type="text"><![CDATA[前言前段时间看的爬虫都是不需要登录直接爬取数据，这回就试试爬取需要登录的网站信息吧，说干就干，直接就拿segmentfault做为目标！ 一、爬虫所需模块 superagent async二、分析我们首先用chrome或者其他浏览器打开segmentfault的主页，找到它的登录接口，点击登录接口，记得把Preserve log勾上，否则跳转之后找不到接口，如图（重要信息打了马赛克）： 显而易见这是一个post请求，三个参数分别是用户名、密码、和是否记住密码的标记。 获取Cookie里的PHPSESSID仔细看请求头header里的cookie，在请求发送的时候就已经有了，我们直接把整个请求头拿过来，就直接用图中的接口和header登录，逐一删除cookie中的项，测试登录结果，最终发现只有PHPSESSID是必须的。这个PHPSESSID如何获取呢？我们可以在登录之前先访问segmentfault主页，将返回的cookie拿到，再在登录的时候加上这个cookie即可。获取cookie的代码如下： 123456789(cb) =&gt; &#123; superagent .get(&apos;https://segmentfault.com&apos;) .end((err, res) =&gt; &#123; if (err) console.log(err) cookie = res.headers[&apos;set-cookie&apos;].join(&apos;,&apos;).split(&apos;;&apos;)[0] // 获取PHPSESSID cb(null) &#125;) &#125; 获取Query String Paramsters里的 _ 参数还有一个参数需要注意：接口中的Query String Paramsters的_参数，那么这个参数是怎么来的呢？在返回的response的header里寻找并没有找到它的踪迹，所以猜想它应该是隐藏在源码里，我们直接在chrome控制台的source下全局搜索_=（source顶部右键选择search in all files即可出现全局搜索框）,逐一排查可能性： 排查过程中发现箭头所指的ajaxSend函数好像和我们需要的有关系：它紧邻的下面的delegate函数内容应该就是登陆有关的内容，通过/api/user/?do=login和submit等就可以清楚的看出，这个函数中的url是从n.attr(&#39;action&#39;)拿到的，猜测这个n.__肯定和请求中的Query String参数脱不了关系。正好这个ajaxSend函数中就有n.__,也正好验证了我们刚才的推测，分析这行代码：1n.url.indexOf(&quot;?&quot;) === -1 ? n.url = n.url + &quot;?_=&quot; + i._ : n.url = n.url + &quot;&amp;_=&quot; + i._ n.url默认是n.url + &quot;?_=&quot; + i._ ，那么这个i.__应该就是最终boss，就在这个文件中找到定义i的代码，如上图箭头所指，继续全局搜索SF.token,最终在index.html中找了生成它的代码，包含在一个script中，如图： 找到来源就简单了，我们仍然是在登录直接先访问主页，将整个主页的html代码拿到，然后将这个script的内容取出来不就行了，哈哈，好开心~获取script的代码如下： 12345678910var cheerio = require(&apos;cheerio&apos;)function getRandom(s) &#123; let $ = cheerio.load(s) let script = $(&apos;script&apos;).eq(8).html() let fn = new Function(&apos;window&apos;, script + &apos;;return window.SF.token&apos;) let token = fn(&#123;&#125;) $ = null return token&#125;exports.getRandom = getRandom 到这里，登录就算完成了一大半了，接下来就是简单的用superagent调用接口啦，这里的请求头出了cookie的其他部分也是必须要设置的，可以直接从浏览器上copy下来，代码如下： 123456789101112131415161718192021222324252627282930313233343536(cb) =&gt; &#123; const username = process.argv[2] const password = process.argv[3] console.log(cookie) console.log(random) let header = &#123; &apos;accept&apos;: &apos;*/*&apos;, &apos;accept-encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;accept-language&apos;: &apos;zh-CN,zh;q=0.9&apos;, &apos;content-length&apos;: &apos;47&apos;, &apos;content-type&apos;: &apos;application/x-www-form-urlencoded; charset=UTF-8&apos;, &apos;cookie&apos;: `PHPSESSID=$&#123;cookie&#125;;`, &apos;origin&apos;: &apos;https://segmentfault.com&apos;, &apos;referer&apos;: &apos;https://segmentfault.com/&apos;, &apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&apos;, &apos;x-requested-with&apos;: &apos;XMLHttpRequest&apos; &#125; superagent .post(`https://segmentfault.com/api/user/login`) .query(&#123;&apos;_&apos;: random&#125;) .set(header) .type(&apos;form&apos;) .send(&#123; username: username, password: password, remember: 1 &#125;) .end(function(err, res) &#123; if (err) &#123; console.log(err.status); &#125; else &#123; console.log(&apos;返回状态码： &apos; + res.status) cb(null) &#125; &#125;) &#125;, 终于返回200了，美滋滋，然后我们继续~比如我我想用代码修改个人主页的个人描述内容，首先我们先找到相关接口，如图： 这个post请求的参数description就是个人描述的所填写的新的内容，我们直接用superagent调用这个接口123456789101112131415161718192021(cb) =&gt; &#123; superagent // 编辑右上角个人说明 .post(&apos;https://segmentfault.com/api/user/homepage/description/edit&apos;) .query(&#123;&apos;_&apos;: random&#125;) .set(header) .type(&apos;form&apos;) .send(&#123; description: &apos;努力coding的小喵~~~&apos; &#125;) .end((err, res) =&gt; &#123; if(err) throw err let result = JSON.parse(res.text) if (result.status === 0) &#123; console.log(&apos;编辑成功&apos;) &#125; else &#123; console.log(&apos;编辑失败：&apos; + result.data) &#125; &#125;) cb(null, 1) &#125; 返回状态码200，然后直接去浏览上的主页刷新下，可以看到个人描述的内容已经成功更新了！ 总结 打开segmentfault主页并登陆，找到登录请求的接口并分析 用node登录之前，先请求主页接口，目的是拿到PHPSESSID和源代码中的生成Query String的函数 带着这两个参数去请求登录接口，记得设置请求头 登录成功之后就可以干任何你想干的事情啦 整个登录过程耗费了很久的时间，有空就研究这个Query String的来源，好不容易登录成功想干点事情，又没注意到设置请求头，以为是sf_remember参数的问题，又折腾了许久，还好，最终总算是成功了！感谢自己的不放弃~ 源码github地址：https://github.com/fighting123/node_login_segmentfault.git]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>node模拟登陆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉勾网爬虫并导出excel]]></title>
    <url>%2F2018%2F07%2F05%2Fnode-crwl-lagou%2F</url>
    <content type="text"><![CDATA[前言 之前断断续续学习了node.js，今天就拿拉勾网练练手，顺便通过数据了解了解最近的招聘行情哈！node方面算是萌新一个吧，希望可以和大家共同学习和进步。 一、概要我们首先需要明确具体的需求： 可以通过node index 城市 职位来爬取相关信息 也可以输入node index start直接爬取我们预定义好的城市和职位数组，循环爬取不同城市的不同职位信息 将最终爬取的结果存储在本地的./data目录下 生成对应的excel文件，并存储到本地 二、爬虫用到的相关模块 fs: 用于对系统文件及目录进行读写操作 async：流程控制 superagent：客户端请求代理模块 node-xlsx：将一定格式的文件导出为excel 三、爬虫主要步骤：初始化项目新建项目目录 在合适的磁盘目录下创建项目目录 node-crwl-lagou 初始化项目 进入node-crwl-lagou文件夹下 执行npm init，初始化package.json文件 安装依赖包 npm install async npm install superagent npm install node-xlsx 命令行输入的处理对于在命令行输入的内容，可以用process.argv来获取，他会返回个数组，数组的每一项就是用户输入的内容。区分node index 地域 职位和node index start两种输入，最简单的就是判断process.argv的长度，长度为四的话，就直接调用爬虫主程序爬取数据，长度为三的话，我们就需要通过预定义的城市和职位数组来拼凑url了，然后利用async.mapSeries循环调用主程序。关于命令分析的主页代码如下： 1234567891011121314151617181920212223if (process.argv.length === 4) &#123; let args = process.argv console.log(&apos;准备开始请求&apos; + args[2] + &apos;的&apos; + args[3] + &apos;职位数据&apos;); requsetCrwl.controlRequest(args[2], args[3])&#125; else if (process.argv.length === 3 &amp;&amp; process.argv[2] === &apos;start&apos;) &#123; let arr = [] for (let i = 0; i &lt; defaultArgv.city.length; i++) &#123; for (let j = 0; j &lt; defaultArgv.position.length; j++) &#123; let obj = &#123;&#125; obj.city = defaultArgv.city[i] obj.position = defaultArgv.position[j] arr.push(obj) &#125; &#125; async.mapSeries(arr, function (item, callback) &#123; console.log(&apos;准备开始请求&apos; + item.city + &apos;的&apos; + item.position + &apos;职位数据&apos;); requsetCrwl.controlRequest(item.city, item.position, callback) &#125;, function (err) &#123; if (err) throw err &#125;)&#125; else &#123; console.log(&apos;请正确输入要爬取的城市和职位，正确格式为：&quot;node index 城市 关键词&quot; 或 &quot;node index start&quot; 例如：&quot;node index 北京 php&quot; 或&quot;node index start&quot;&apos;)&#125; 预定义好的城市和职位数组如下： 1234&#123; &quot;city&quot;: [&quot;北京&quot;,&quot;上海&quot;,&quot;广州&quot;,&quot;深圳&quot;,&quot;杭州&quot;,&quot;南京&quot;,&quot;成都&quot;,&quot;西安&quot;,&quot;武汉&quot;,&quot;重庆&quot;], &quot;position&quot;: [&quot;前端&quot;,&quot;java&quot;,&quot;php&quot;,&quot;ios&quot;,&quot;android&quot;,&quot;c++&quot;,&quot;python&quot;,&quot;.NET&quot;]&#125; 接下来就是爬虫主程序部分的分析了。 分析页面，找到请求地址首先我们打开拉勾网首页，输入查询信息（比如node），然后查看控制台，找到相关的请求，如图： 这个post请求https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false就是我们所需要的，通过三个请求参数来获取不同的数据，简单的分析就可得知：参数first是标注当前是否是第一页，true为是，false为否；参数pn是当前的页码；参数kd是查询输入的内容。 通过superagent请求数据首先需要明确得是，整个程序是异步的，我们需要用async.series来依次调用。查看分析返回的response： 可以看到content.positionResult.totalCount就是我们所需要的总页数我们用superagent直接调用post请求，控制台会提示如下信息： {&apos;success&apos;: False, &apos;msg&apos;: &apos;您操作太频繁,请稍后再访问&apos;, &apos;clientIp&apos;: &apos;122.xxx.xxx.xxx&apos;} 这其实是反爬虫策略之一，我们只需要给其添加一个请求头即可，请求头的获取方式很简单，如下： 然后在用superagent调用post请求，主要代码如下： 12345678910111213141516171819202122// 先获取总页数 (cb) =&gt; &#123; superagent .post(`https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&amp;city=$&#123;city&#125;&amp;kd=$&#123;position&#125;&amp;pn=1`) .send(&#123; &apos;pn&apos;: 1, &apos;kd&apos;: position, &apos;first&apos;: true &#125;) .set(options.options) .end((err, res) =&gt; &#123; if (err) throw err // console.log(res.text) let resObj = JSON.parse(res.text) if (resObj.success === true) &#123; totalPage = resObj.content.positionResult.totalCount; cb(null, totalPage); &#125; else &#123; console.log(`获取数据失败:$&#123;res.text&#125;&#125;`) &#125; &#125;) &#125;, 拿到总页数后，我们就可以通过总页数/15获取到pn参数，循环生成所有url并存入urls中： 1234567(cb) =&gt; &#123; for (let i=0;Math.ceil(i&lt;totalPage/15);i++) &#123; urls.push(`https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&amp;city=$&#123;city&#125;&amp;kd=$&#123;position&#125;&amp;pn=$&#123;i&#125;`) &#125; console.log(`$&#123;city&#125;的$&#123;position&#125;职位共$&#123;totalPage&#125;条数据，$&#123;urls.length&#125;页`); cb(null, urls); &#125;, 有了所有的url，在想爬到所有的数据就不是难事了，继续用superagent的post方法循环请求所有的url，每一次获取到数据后，在data目录下创建json文件，将返回的数据写入。这里看似简单，但是有两点需要注意： 为了防止并发请求太多而导致被封IP：循环url时候需要使用async.mapLimit方法控制并发为3， 每次请求完都要过两秒在发送下一次的请求 在async.mapLimit的第四个参数中，需要通过判断调用主函数的第三个参数是否存在来区分一下是那种命令输入，因为对于node index start这个命令，我们使用得是async.mapSeries，每次调用主函数都传递了(city, position, callback)，所以如果是node index start的话，需要在每次获取数据完后将null传递回去，否则无法进行下一次循环 主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 控制并发为3 (cb) =&gt; &#123; async.mapLimit(urls, 3, (url, callback) =&gt; &#123; num++; let page = url.split(&apos;&amp;&apos;)[3].split(&apos;=&apos;)[1]; superagent .post(url) .send(&#123; &apos;pn&apos;: totalPage, &apos;kd&apos;: position, &apos;first&apos;: false &#125;) .set(options.options) .end((err, res) =&gt; &#123; if (err) throw err let resObj = JSON.parse(res.text) if (resObj.success === true) &#123; console.log(`正在抓取第$&#123;page&#125;页，当前并发数量：$&#123;num&#125;`); if (!fs.existsSync(&apos;./data&apos;)) &#123; fs.mkdirSync(&apos;./data&apos;); &#125; // 将数据以.json格式储存在data文件夹下 fs.writeFile(`./data/$&#123;city&#125;_$&#123;position&#125;_$&#123;page&#125;.json`, res.text, (err) =&gt; &#123; if (err) throw err; // 写入数据完成后，两秒后再发送下一次请求 setTimeout(() =&gt; &#123; num--; console.log(`第$&#123;page&#125;页写入成功`); callback(null, &apos;success&apos;); &#125;, 2000); &#125;); &#125; &#125;) &#125;, (err, result) =&gt; &#123; if (err) throw err; // 这个arguments是调用controlRequest函数的参数，可以区分是那种爬取（循环还是单个） if (arguments[2]) &#123; ok = 1; &#125; cb(null, ok) &#125;) &#125;, () =&gt; &#123; if (ok) &#123; setTimeout(function () &#123; console.log(`$&#123;city&#125;的$&#123;position&#125;数据请求完成`); indexCallback(null); &#125;, 5000); &#125; else &#123; console.log(`$&#123;city&#125;的$&#123;position&#125;数据请求完成`); &#125; // exportExcel.exportExcel() // 导出为excel &#125; 导出的json文件如下： json文件导出为excel将json文件导出为excel有多种方式，我使用的是node-xlsx这个node包，这个包需要将数据按照固定的格式传入，然后导出即可，所以我们首先做的就是先拼出其所需的数据格式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869function exportExcel() &#123; let list = fs.readdirSync(&apos;./data&apos;) let dataArr = [] list.forEach((item, index) =&gt; &#123; let path = `./data/$&#123;item&#125;` let obj = fs.readFileSync(path, &apos;utf-8&apos;) let content = JSON.parse(obj).content.positionResult.result let arr = [[&apos;companyFullName&apos;, &apos;createTime&apos;, &apos;workYear&apos;, &apos;education&apos;, &apos;city&apos;, &apos;positionName&apos;, &apos;positionAdvantage&apos;, &apos;companyLabelList&apos;, &apos;salary&apos;]] content.forEach((contentItem) =&gt; &#123; arr.push([contentItem.companyFullName, contentItem.phone, contentItem.workYear, contentItem.education, contentItem.city, contentItem.positionName, contentItem.positionAdvantage, contentItem.companyLabelList.join(&apos;,&apos;), contentItem.salary]) &#125;) dataArr[index] = &#123; data: arr, name: path.split(&apos;./data/&apos;)[1] // 名字不能包含 \ / ? * [ ] &#125; &#125;)// 数据格式// var data = [// &#123;// name : &apos;sheet1&apos;,// data : [// [// &apos;ID&apos;,// &apos;Name&apos;,// &apos;Score&apos;// ],// [// &apos;1&apos;,// &apos;Michael&apos;,// &apos;99&apos;//// ],// [// &apos;2&apos;,// &apos;Jordan&apos;,// &apos;98&apos;// ]// ]// &#125;,// &#123;// name : &apos;sheet2&apos;,// data : [// [// &apos;AA&apos;,// &apos;BB&apos;// ],// [// &apos;23&apos;,// &apos;24&apos;// ]// ]// &#125;// ]// 写xlsx var buffer = xlsx.build(dataArr) fs.writeFile(&apos;./result.xlsx&apos;, buffer, function (err) &#123; if (err) throw err; console.log(&apos;Write to xls has finished&apos;);// 读xlsx// var obj = xlsx.parse(&quot;./&quot; + &quot;resut.xls&quot;);// console.log(JSON.stringify(obj)); &#125; );&#125; 导出的excel文件如下，每一页的数据都是一个sheet，比较清晰明了： 我们可以很清楚的从中看出目前西安.net的招聘情况，之后也可以考虑用更形象的图表方式展示爬到的数据，应该会更加直观！ 总结其实整个爬虫过程并不复杂，注意就是注意的小点很多，比如async的各个方法的使用以及导出设置header等，总之，也是收获满满哒！ 源码gitbug地址： https://github.com/fighting123/node_crwl_lagou]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>免登陆的爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webpack初识]]></title>
    <url>%2F2018%2F03%2F19%2Fwebpack%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[安装webpack123456789//全局安装npm install -g webpack//安装到项目目录npm install --save-dev webpack//全局安装（同时需要安装webpack-cli，否则提示The CLI moved into a separate package: webpack-cli.）npm install webpack-cli -g//安装到项目目录npm install --save-dev webpack-cli 区别：安装到项目目录运行时需要加上在node_modules中的地址，如：node_modules/.bin/webpack 使用webpack前的准备工作创建如图所示的文件： 首先用npm init生成package.json文件，然后创建文件，public是存放浏览器读取的文件，app是存放未打包前的模块文件 Greeter.js模块: 1234567const greeter = () =&gt; &#123; let greet = document.createElement(&apos;div&apos;) greet.textContent = &apos;hello&apos; return greet&#125;module.exports = greeter() main.js引入模块的文件： 12let greeter = require(&apos;./Greeter&apos;)document.querySelector(&quot;#body&quot;).appendChild(greeter) index.html主页面: 12345678910111213&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;webpack_sample_practice&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;body&quot;&gt;&lt;/body&gt;&lt;script src=&quot;bundle.js&quot;&gt;&lt;/script&gt;&lt;/html&gt; bundle.js打包后存储的文件（index.html引入的文件） （一）使用命启动令webpack1webpack app/main.js public/bundle.js // webpack 入口文件 打包后文件 这时会提示warning，这是因为没有设置mode，每次启动的时候：123webpack app/main.js public/bundle.js --mode development（未压缩）// 或者webpack app/main.js public/bundle.js --mode production（压缩过） 为方便起见，我们将这两句命令放入package.json文件中：1234&quot;scripts&quot;: &#123; &quot;dev&quot;: &quot;webpack app/main.js public/bundle.js --mode development&quot;,//（未压缩） &quot;build&quot;: &quot;webpack app/main.js public/bundle.js --mode production&quot; //（压缩过） &#125;, 这样，在每次启动是只需要输入npm run dev 或者npm run build 最后，打开浏览器页面就可以访问啦！ （二）通过配置webpack.config.js来使用webpack我们也可以通过更加方便简洁的方式使用webpack，这样也比较不容易出错。 在主目录新建webpack.config.js配置文,先简单的只配置下入口和出口文件目录: 1234567module.exports = &#123; entry: __dirname + &quot;/app/main.js&quot;,//已多次提及的唯一入口文件 output: &#123; path: __dirname + &quot;/public&quot;,//打包后的文件存放的地方 filename: &quot;bundle.js&quot;//打包后输出文件的文件名 &#125;&#125; 同意为方便我们配置package.json文件： 1234&quot;scripts&quot;: &#123; &quot;dev&quot;: &quot;webpack --mode development&quot;, &quot;build&quot;: &quot;webpack --mode production&quot; &#125;, 输入npm run dev 或者 npm run build同样可以正常启动啦！ 参考：https://www.jianshu.com/p/42e11515c10f]]></content>
      <categories>
        <category>webpack</category>
      </categories>
      <tags>
        <tag>打包，webpack初识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo基本操作]]></title>
    <url>%2F2018%2F03%2F15%2Fhello-world-2%2F</url>
    <content type="text"><![CDATA[创建文章1$ hexo new "My New Post" More info: Writing 本地运行1$ hexo server More info: Server 编译1$ hexo generate More info: Generating 发布1$ hexo deploy]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>hexo基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建简记]]></title>
    <url>%2F2018%2F03%2F14%2Fhello-world%2F</url>
    <content type="text"><![CDATA[博客搭建步骤简记 按照教程配好基础的博客并修改个人信息，可参考： https://www.jianshu.com/p/d49e4684e62b https://hexo.io/zh-cn/docs/configuration.html https://www.jianshu.com/p/9f0e90cc32c2 更换主题为next 添加分类。标签功能 添加搜索功能，next官网上有很多第三方搜索服务，我用的是Local Search，具体可以按照教程来 添加统计，同上，我用的LeanCloud和不蒜子 添加评论功能，同上，DISQUS官网进不去，所以我用的是LeanCloud 首页文章以摘要形式显示：将主题设置的auto_excerpt的enable为true 设置首页文章显示篇数(参考：http://www.jeyzhang.com/next-theme-personal-settings.html) 设置404页面,用的是腾讯公益404页面(直接在站点的source目录下新建404.html) 设置头像(两张存储位置对应不用的文件夹名称，需要注意，然后将主题的avatar的注释放开并且填写对应位置) 设置icon，在阿里妈妈矢量库寻找合适图像，下载1616和3232的png格式，将next主题下的favicon图片更换掉 解决github+Hexo的博客多终端同步问题(https://www.jianshu.com/p/6fb0b287f950) 剩下的各种详情都参照第一条发布文章要用gitbash，用终端会报错发布文章完成后记得上传到git的hexo分支，保存源代码]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>hexo搭建简记</tag>
      </tags>
  </entry>
</search>
